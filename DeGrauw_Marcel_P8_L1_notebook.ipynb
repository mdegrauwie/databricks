{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b6de9a38-66d0-42c6-bbd3-dc2783426813",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Projet 8: Déployez un modèle dans le cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d59a6d91-b3f7-4715-a9a1-db45334e285f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53373914-86af-41e7-bd63-e2eb06ed6ab4",
     "showTitle": false,
     "title": ""
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install Pandas \n",
    "!pip install pillow \n",
    "!pip install tensorflow \n",
    "!pip install pyspark \n",
    "!pip install pyarrow\n",
    "!pip install mlflow\n",
    "!pip install hyperopt\n",
    "!pip install databricks-cli\n",
    "!pip install fastparquet\n",
    "!pip install opencv-python\n",
    "!pip install xgboost\n",
    "\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe3abe8c-5838-465b-bca3-653829f6ab90",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Import Librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42aafbc2-2b95-4a13-a0f1-2eec9d7afe23",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import io as io\n",
    "import seaborn as sns\n",
    "import math as math\n",
    "import cv2 as cv2\n",
    "import timeit as timeit\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import pickle as pickle\n",
    "import time as time\n",
    "import logging as logging\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras\n",
    "import os as os\n",
    "import re as re\n",
    "import IPython.display as display\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "\n",
    "from PIL import Image\n",
    "from PIL import Image, ImageOps, ImageEnhance\n",
    "from PIL import ImageFilter\n",
    "\n",
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras import Model\n",
    "from pyspark.sql.functions import col, pandas_udf, PandasUDFType, element_at, split\n",
    "from pyspark.sql import SparkSession\n",
    "from tensorflow.keras.utils import load_img, img_to_array\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.applications.vgg16 import decode_predictions\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import metrics as kmetrics\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from os import listdir\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4afabc7d-cd87-4b0a-887b-d6426277e4fb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Initialisation\n",
    "\n",
    "Pour la 1e version, qui sera déployé en locale, nous partons du principe que les données sont stockées dans le même répertoire que le notebook. Nous n'utilisons qu'un extrait de 50 images à traiter dans une première version, qui sera déployé en local. L'extrait des images à charger est stockée dans le dossier Test1. Nous enregistrerons le résultat de notre traitement dans le dossier \"Results_Local\". Avant de commencer il est nécesair d'installer et de deployer Apache Spark sur son poste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c44f5bb9-b419-477f-a3ee-cf25ea1cad58",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%fs mkdirs /FileStore/data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f97271c-c531-4fd7-b9b4-434f9f5d2297",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%fs mkdirs /FileStore/data/Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40a97296-3e05-4b75-a65b-d26f3f26c7f3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%fs mkdirs /FileStore/data/Train/fraise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4bab8c9-0575-43e2-9d2c-75def7093d33",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%fs mkdirs /FileStore/data/Train/figue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ccc794c-53f8-4bd1-9306-3e4d177ca3e0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%fs mkdirs /FileStore/data/Train/banana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "938d0223-7cdc-4003-9e46-8517d3b0d0fa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%fs mkdirs /FileStore/data/Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "005e36ce-caaf-4b19-bf14-034c3b8ae024",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%fs mkdirs /FileStore/data/Test/fraise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f320f1d9-6904-4d2a-a39b-079adf8c2fd4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%fs mkdirs /FileStore/data/Test/figue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5efb5d2-9584-474b-a6f2-6aeb05ed6e83",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%fs mkdirs /FileStore/data/Test/banana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0e2889a-3373-42f6-868f-0053595d352f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%fs mkdirs /FileStore/data/Results_Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3769c413-bb11-47d3-bb71-a41c300fe987",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%fs mkdirs /FileStore/data/Results_Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53c136d2-cd34-4cc7-ade9-9d56eda60b43",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Définition  PATH pour charger les images et enregistrer les résultats\n",
    "\n",
    "PATH = '/FileStore'\n",
    "PATH_Train = PATH+'/data/Train'\n",
    "PATH_Test = PATH+'/data/Test'\n",
    "PATH_Train_Results = PATH+'/data/Results_Train'\n",
    "PATH_Test_Results = PATH+'/data/Results_Test'\n",
    "print('PATH:        '+\\\n",
    "      PATH+'\\nPATH_Train:  '+\\\n",
    "      PATH_Train+'\\nPATH_Train_Results: '+PATH_Train_Results)\n",
    "print('PATH:        '+\\\n",
    "      PATH+'\\nPATH_Test:   '+\\\n",
    "      PATH_Test+'\\nPATH_Test_Results: '+PATH_Test_Results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "90417e52-e7ca-4139-bf55-f86f4b54f9f8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Comme decrit plus haut, l’application Spark est contrôlée grâce à un processus de pilotage (driver process) appelé SparkSession. Une instance de SparkSession est la façon dont Spark exécute les fonctions définies par l’utilisateur dans l’ensemble du cluster. Une SparkSession correspond toujours à une application Spark. Dans notre example nous allons créer une session Spark en spécifiant les paramètres suivants:\n",
    "\n",
    " 1. appName: un nom pour l'application, qui sera affichée dans l'interface utilisateur Web Spark \"**P8**\"\n",
    " 2. master: un paramètre pour indiquer que l'application doit s'exécuter localement. \n",
    " 3. config: une option de configuration supplémentaire permettant d'utiliser le **format \"parquet\"** \n",
    " 4. getOrCreate: un paramètre pour obtenir une session spark existante ou si aucune n'existe, en créer une nouvelle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7938bb60-88e8-4078-ad6c-292083946762",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Traitement des données\n",
    "\n",
    "<u>Dans la suite de notre flux de travail, \n",
    "nous allons successivement</u> :\n",
    "1. Préparer nos données\n",
    "    1. Importer les images dans un dataframe **pandas UDF**\n",
    "    2. Associer aux images leur **label**\n",
    "    3. Préprocesser en **redimensionnant nos images pour \n",
    "       qu'elles soient compatibles avec notre modèle**\n",
    "2. Préparer notre modèle\n",
    "    1. Importer le modèle **MobileNetV2**\n",
    "    2. Créer un **nouveau modèle** dépourvu de la dernière couche de MobileNetV2\n",
    "3. Définir le processus de chargement des images et l'application \n",
    "   de leur featurisation à travers l'utilisation de pandas UDF\n",
    "3. Exécuter les actions d'extraction de features\n",
    "4. Enregistrer le résultat de nos actions\n",
    "5. Tester le bon fonctionnement en chargeant les données enregistrées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2318c73a-6e13-4bfd-a93c-fc87f4ae056d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "images_Train = spark.read.format(\"binaryFile\") \\\n",
    "  .option(\"pathGlobFilter\", \"*.jpg\") \\\n",
    "  .option(\"recursiveFileLookup\", \"true\") \\\n",
    "  .load(PATH_Train)\n",
    "  \n",
    "images_Test = spark.read.format(\"binaryFile\") \\\n",
    "  .option(\"pathGlobFilter\", \"*.jpg\") \\\n",
    "  .option(\"recursiveFileLookup\", \"true\") \\\n",
    "  .load(PATH_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83b279cb-cb8d-48fc-a05a-ff3498a117b3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "images_Train = images_Train.withColumn('label', element_at(split(images_Train['path'], '/'),-2))\n",
    "print(images_Train.select('path','label').show(10,False))\n",
    "\n",
    "images_Test = images_Test.withColumn('label', element_at(split(images_Test['path'], '/'),-2))\n",
    "print(images_Test.select('path','label').show(10,False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0b46f9ca-ae25-45b2-a3dc-dc0631deb449",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Préparation du modèle\n",
    "\n",
    "Je vais utiliser la technique du **transfert learning** pour extraire les features des images.\n",
    "J'ai choisi d'utiliser le modèle **MobileNetV2** pour sa rapidité d'exécution comparée \n",
    "à d'autres modèles comme *VGG16* par exemple.\n",
    "\n",
    "Pour en savoir plus sur la conception et le fonctionnement de MobileNetV2, \n",
    "je vous invite à lire [cet article](https://towardsdatascience.com/review-mobilenetv2-light-weight-model-image-classification-8febb490e61c).\n",
    "\n",
    "<u>Voici le schéma de son architecture globale</u> : \n",
    "\n",
    "![Architecture de MobileNetV2](img/mobilenetv2_architecture.png)\n",
    "\n",
    "Il existe une dernière couche qui sert à classer les images \n",
    "selon 1000 catégories que nous ne voulons pas utiliser.\n",
    "L'idée dans ce projet est de récupérer le **vecteur de caractéristiques \n",
    "de dimensions (1,1,1280)** qui servira, plus tard, au travers d'un moteur \n",
    "de classification à reconnaitre les différents fruits du jeu de données.\n",
    "\n",
    "Comme d'autres modèles similaires, **MobileNetV2**, lorsqu'on l'utilise \n",
    "en incluant toutes ses couches, attend obligatoirement des images \n",
    "de dimension (224,224,3). Nos images étant toutes de dimension (100,100,3), \n",
    "nous devrons simplement les **redimensionner** avant de les confier au modèle.\n",
    "\n",
    "<u>Dans l'odre</u> :\n",
    " 1. Nous chargeons le modèle **MobileNetV2** avec les poids **précalculés** \n",
    "    issus d'**imagenet** et en spécifiant le format de nos images en entrée\n",
    " 2. Nous créons un nouveau modèle avec:\n",
    "  - <u>en entrée</u> : l'entrée du modèle MobileNetV2\n",
    "  - <u>en sortie</u> : l'avant dernière couche du modèle MobileNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b5792af-9dbe-4c35-9f67-9c2df64aa810",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model = MobileNetV2(weights='imagenet',\n",
    "                    include_top=True,\n",
    "                    input_shape=(224, 224, 3))\n",
    "new_model = Model(inputs=model.input,\n",
    "                  outputs=model.layers[-2].output)\n",
    "new_model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "12c535b9-111c-4052-92a1-7609e0225b87",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Tous les workeurs doivent pouvoir accéder au modèle ainsi qu'à ses poids. \n",
    "Une bonne pratique consiste à charger le modèle sur le driver puis à diffuser \n",
    "ensuite les poids aux différents workeurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0938452a-2eb1-4dab-8cac-9f12baab8f52",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "brodcast_weights = sc.broadcast(new_model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e0201d0-1225-46ba-9102-dd4fd875ed24",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def model_fn():\n",
    "    \"\"\"\n",
    "    Returns a MobileNetV2 model with top layer removed \n",
    "    and broadcasted pretrained weights.\n",
    "    \"\"\"\n",
    "    model = MobileNetV2(weights='imagenet',\n",
    "                        include_top=True,\n",
    "                        input_shape=(224, 224, 3))\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "    new_model = Model(inputs=model.input,\n",
    "                  outputs=model.layers[-2].output)\n",
    "    new_model.set_weights(brodcast_weights.value)\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c9ab20b-3aa8-4a99-95f2-c20bcd2f979f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Preprocessing: resize the images to 224*224\n",
    "def preprocess(content):\n",
    "    \"\"\"\n",
    "    Preprocesses raw image bytes for prediction.\n",
    "    \"\"\"\n",
    "    img = Image.open(io.BytesIO(content)).resize([224, 224])\n",
    "    arr = img_to_array(img)\n",
    "    return preprocess_input(arr)\n",
    "\n",
    "def featurize_series(model, content_series):\n",
    "    \"\"\"\n",
    "    Featurize a pd.Series of raw images using the input model.\n",
    "    :return: a pd.Series of image features\n",
    "    \"\"\"\n",
    "    input = np.stack(content_series.map(preprocess))\n",
    "    preds = model.predict(input)\n",
    "    # For some layers, output features will be multi-dimensional tensors.\n",
    "    # We flatten the feature tensors to vectors for easier storage in Spark DataFrames.\n",
    "    output = [p.flatten() for p in preds]\n",
    "    return pd.Series(output)\n",
    "\n",
    "@pandas_udf('array<float>', PandasUDFType.SCALAR_ITER)\n",
    "def featurize_udf(content_series_iter):\n",
    "    '''\n",
    "    This method is a Scalar Iterator pandas UDF wrapping our featurization function.\n",
    "    The decorator specifies that this returns a Spark DataFrame column of type ArrayType(FloatType).\n",
    "\n",
    "    :param content_series_iter: This argument is an iterator over batches of data, where each batch\n",
    "                              is a pandas Series of image data.\n",
    "    '''\n",
    "    # With Scalar Iterator pandas UDFs, we can load the model once and then re-use it\n",
    "    # for multiple data batches.  This amortizes the overhead of loading big models.\n",
    "    model = model_fn()\n",
    "    for content_series in content_series_iter:\n",
    "        yield featurize_series(model, content_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f90d3a86-8181-4a31-ab74-641ad2f79afb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Extraction de features\n",
    "\n",
    "Les Pandas UDF, sur de grands enregistrements (par exemple, de très grandes images), \n",
    "peuvent rencontrer des erreurs de type Out Of Memory (OOM).\n",
    "Si vous rencontrez de telles erreurs dans la cellule ci-dessous, \n",
    "essayez de réduire la taille du lot Arrow via 'maxRecordsPerBatch'\n",
    "\n",
    "Je n'utiliserai pas cette commande dans ce projet \n",
    "et je laisse donc la commande en commentaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b31a13e-e19e-4d86-aeaa-57ac1a65e40a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# spark.conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"1024\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "25e219d7-285a-4164-92c8-594f203ca296",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Nous pouvons maintenant exécuter la featurisation sur l'ensemble de notre DataFrame Spark.\n",
    "<u>REMARQUE</u> : Cela peut prendre beaucoup de temps, tout dépend du volume de données à traiter. \n",
    "\n",
    "Notre jeu de données de **Test** contient **22819 images**. \n",
    "Cependant, dans l'exécution en mode **local**, \n",
    "nous <u>traiterons un ensemble réduit de **330 images**</u>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "312c29dc-886f-47db-be2a-de7987e0ec25",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "features_df_Train = images_Train.repartition(20).select(col(\"path\"),\n",
    "                                            col(\"label\"),\n",
    "                                            featurize_udf(\"content\").alias(\"features\")\n",
    "                                           )\n",
    "features_df_Test = images_Test.repartition(20).select(col(\"path\"),\n",
    "                                            col(\"label\"),\n",
    "                                            featurize_udf(\"content\").alias(\"features\")\n",
    "                                           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9038257a-1d07-4f1e-8a25-d6341f7164a3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<u>Rappel du PATH où seront inscrits les fichiers au format \"**parquet**\" \n",
    "contenant nos résultats, à savoir, un DataFrame contenant 3 colonnes</u> :\n",
    " 1. Path des images\n",
    " 2. Label de l'image\n",
    " 3. Vecteur de caractéristiques de l'image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3766ca4-5afc-4efb-b275-7c48dcf9d9f4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(PATH_Train_Results)\n",
    "print(PATH_Test_Results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dfc9551a-4ed9-4e19-bfc9-4f475ef0ed20",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<u>Enregistrement des données traitées au format \"**parquet**\"</u> :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7c31948-cc60-4ec6-8a5d-1b85416a363f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "features_df_Train.write.mode(\"overwrite\").parquet(PATH_Train_Results)\n",
    "features_df_Test.write.mode(\"overwrite\").parquet(PATH_Test_Results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ec2e3a90-ad5a-455b-9e15-ca45b1553f4f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Chargement des données enregistrées et validation du résultat\n",
    "\n",
    "<u>On charge les données fraichement enregistrées dans un **DataFrame Pandas**</u> :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f7a1f1c-2d04-4fdd-98c6-32b6eacfe42e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_Train = spark.read.parquet(PATH_Train_Results)\n",
    "df_Train.show()\n",
    "df_Train = df_Train.toPandas()\n",
    "df_Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdcdc3c0-d8cc-4100-8bf9-0e438c452621",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_Test = spark.read.parquet(PATH_Test_Results)\n",
    "df_Test.show()\n",
    "df_Test = df_Test.toPandas()\n",
    "df_Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "963439e9-f22d-4520-a0c0-0621f2ae9611",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Train: \",df_Train.loc[0,'features'].shape)\n",
    "print(\"Test: \",df_Test.loc[0,'features'].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02ea4d12-9c92-4a56-b5ed-1a17d8d2e422",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Train features vector: \\n\")\n",
    "train_images=np.array(df_Train['features'].values.tolist())\n",
    "print(train_images)\n",
    "print(\"\\n\")\n",
    "print(\"Test features vector: \\n\")\n",
    "test_images=np.array(df_Test['features'].values.tolist())\n",
    "print(test_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3ee64950-389c-4538-8433-c70c029ec815",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Nous venons de valider le processus sur un jeu de données allégé en local \n",
    "où nous avons simulé un cluster de machines en répartissant la charge de travail \n",
    "sur différents cœurs de processeur au sein d'une même machine.\n",
    "\n",
    "Nous allons maintenant généraliser le processus en déployant notre solution \n",
    "sur un réel cluster de machines et nous travaillerons désormais sur la totalité \n",
    "des 22819 images de notre dossier \"Test\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87019149-f7fc-4fff-a1cd-5b96dd268993",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_labels=df_Train['label']\n",
    "test_labels=df_Test['label']\n",
    "\n",
    "#Encode labels from text to integers.\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "le.fit(train_labels)\n",
    "train_labels_encoded = le.transform(train_labels)\n",
    "le.fit(test_labels)\n",
    "test_labels_encoded = le.transform(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc8c4c5b-5280-4b50-8f46-405e7b3cb88e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#Split data into test and train datasets (already split but assigning to meaningful convention)\n",
    "X_train, y_train, X_test, y_test = train_images, train_labels_encoded, test_images, test_labels_encoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "35fffa5b-f1be-44f3-aa2a-9702f091d099",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4502474-4c81-445e-b6f3-7688e53c789c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd8698bc-cd36-4ed4-9438-f389bc2e667f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "# Enable MLflow autologging for this notebook\n",
    "mlflow.autolog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "573cc547-7104-405c-9558-d88539921180",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name='gradient_boost') as run:\n",
    "    \n",
    "# Train the XGBoost model on the training data \n",
    "    import xgboost as xgb\n",
    "    models = xgb.XGBClassifier(random_state=42)\n",
    "    models.fit(X_train, y_train)\n",
    "\n",
    "    #Now predict the class using the test dataset by means of the trained XGBoost model. \n",
    "    prediction = models.predict(X_test)\n",
    "\n",
    "    #Transform the integer label back to its original name by means of the label encoder inverse function\n",
    "    prediction_decoded = le.inverse_transform(prediction)\n",
    "\n",
    "    #Print overall accuracy\n",
    "    from sklearn import metrics\n",
    "    print (\"Accuracy = \", metrics.accuracy_score(test_labels, prediction_decoded))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91722564-992e-4f2e-af7c-c88a3b95ddb7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Confusion Matrix - verify accuracy of each class\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(test_labels, prediction_decoded)\n",
    "sns.heatmap(cm, fmt='g', annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3c43fd7-09fe-43ff-a3d0-cfcccbae5659",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import sklearn.datasets\n",
    "import sklearn.metrics\n",
    "import sklearn.model_selection\n",
    "import sklearn.ensemble\n",
    "from hyperopt import fmin, tpe, hp, SparkTrials, Trials, STATUS_OK\n",
    "from hyperopt.pyll import scope\n",
    "\n",
    "# Define the search space to explore\n",
    "search_space = {\n",
    "  'n_estimators': scope.int(hp.quniform('n_estimators', 20, 1000, 1)),\n",
    "  'learning_rate': hp.loguniform('learning_rate', -3, 0),\n",
    "  'max_depth': scope.int(hp.quniform('max_depth', 2, 5, 1)),\n",
    "}\n",
    " \n",
    "def train_model(params):\n",
    "  # Enable autologging on each worker\n",
    "  mlflow.autolog()\n",
    "  with mlflow.start_run(nested=True):\n",
    "    model_hp = xgb.XGBClassifier(\n",
    "      random_state=0,\n",
    "      **params\n",
    "    )\n",
    "    model_hp.fit(X_train, y_train)\n",
    "    prediction = model_hp.predict(X_test)\n",
    "    prediction_decoded = le.inverse_transform(prediction)\n",
    "    # Tune based on the test AUC\n",
    "    # In production settings, you could use a separate validation set instead\n",
    "    accuracy = sklearn.metrics.accuracy_score(test_labels, prediction_decoded)\n",
    "    mlflow.log_metric('test_accuracy', accuracy)\n",
    "    \n",
    "    # Set the loss to -1*auc_score so fmin maximizes the auc_score\n",
    "    return {'status': STATUS_OK, 'loss': -1*accuracy}\n",
    " \n",
    "# SparkTrials distributes the tuning using Spark workers\n",
    "# Greater parallelism speeds processing, but each hyperparameter trial has less information from other trials\n",
    "# On smaller clusters or Databricks Community Edition try setting parallelism=2\n",
    "spark_trials = SparkTrials(\n",
    "  parallelism=8\n",
    ")\n",
    " \n",
    "with mlflow.start_run(run_name='gb_hyperopt') as run:\n",
    "  # Use hyperopt to find the parameters yielding the highest AUC\n",
    "  best_params = fmin(\n",
    "    fn=train_model, \n",
    "    space=search_space, \n",
    "    algo=tpe.suggest, \n",
    "    max_evals=8,\n",
    "    trials=spark_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8bb79950-ba4e-4bb8-9596-e877d04fe210",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sort runs by their test auc; in case of ties, use the most recent run\n",
    "best_run = mlflow.search_runs(\n",
    "  order_by=['metrics.test_accuracy DESC', 'start_time DESC'],\n",
    "  max_results=10,\n",
    ").iloc[0]\n",
    "print('Best Run')\n",
    "print('Accuracy: {}'.format(best_run[\"metrics.test_accuracy\"]))\n",
    "#print('Num Estimators: {}'.format(best_run[\"params.n_estimators\"]))\n",
    "print('Max Depth: {}'.format(best_run[\"params.max_depth\"]))\n",
    "print('Learning Rate: {}'.format(best_run[\"params.learning_rate\"]))\n",
    " \n",
    "best_model_pyfunc = mlflow.pyfunc.load_model(\n",
    "  'runs:/{run_id}/model'.format(\n",
    "    run_id=best_run.run_id\n",
    "  )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5508c33-d996-47af-a6ee-fcf4e8b46cba",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name='gradient_boost') as run:\n",
    "    \n",
    "# Train the XGBoost model on the training data \n",
    "    import xgboost as xgb\n",
    "    models = xgb.XGBClassifier(random_state=42, max_depth=4, learning_rate=0.22334119485819626)\n",
    "    models.fit(X_train, y_train)\n",
    "\n",
    "    #Now predict the class using the test dataset by means of the trained XGBoost model. \n",
    "    prediction = models.predict(X_test)\n",
    "\n",
    "    #Transform the integer label back to its original name by means of the label encoder inverse function\n",
    "    prediction_decoded = le.inverse_transform(prediction)\n",
    "\n",
    "    #Print overall accuracy\n",
    "    from sklearn import metrics\n",
    "    print (\"Accuracy = \", metrics.accuracy_score(test_labels, prediction_decoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "96d6911c-114c-4853-86cc-6898c7e722e8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1429974172525052,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "DeGrauw_Marcel_P8_L1_notebook",
   "notebookOrigID": 2641570365049154,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "432.4px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
